{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naru289/Assignment-17/blob/main/M3_AST_17_Bidirectional_GRU_for_Sentence_Classification_C%20copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4Nwm4FK3wgU"
      },
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment : Natural Language Processing - II (Bidirectional GRU for Sentence Classification)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSEE2MI6FWZ1"
      },
      "source": [
        "### Learning Objectives:\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        " \n",
        "*  generate vector representation of words in the data using Glove embeddings\n",
        "*  implement the multi-layer bidirectional GRU (Gated\n",
        "Recurrent Unit) for solving the sentence classification problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWMVQWk58aXm"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"2237180\" #@param {type:\"string\"}"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwqosl928dBA"
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"6366871391\" #@param {type:\"string\"}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "GXbNUL2L6LoU",
        "outputId": "50d03bf2-0238-4e60-f1ad-e1b73d715e52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "ipython = get_ipython()\n",
        "  \n",
        "notebook= \"M3_AST_17_Bidirectional_GRU_for_Sentence_Classification_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "    ipython.magic(\"sx wget -qq https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/glove.6B.zip\")\n",
        "    ipython.magic(\"sx unzip glove.6B.zip\")\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/rt-polarity.zip\")\n",
        "    ipython.magic(\"sx unzip rt-polarity.zip\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "    \n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None        \n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "    \n",
        "    elif getAnswer1() and getAnswer2() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n",
        "              \"answer1\" : Answer1, \"answer2\" : Answer2, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None   \n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://dlfa-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "    \n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional: \n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional  \n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "  \n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "  \n",
        "  \n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "  \n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer1():\n",
        "  try:\n",
        "    if not Answer1:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer1\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 1\")\n",
        "    return None\n",
        "\n",
        "def getAnswer2():\n",
        "  try:\n",
        "    if not Answer2:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer2\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 2\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getId():\n",
        "  try: \n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup \n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup() \n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=2237180&recordId=1635\"></script>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8amkxNZMOBE"
      },
      "source": [
        "### Dataset Description\n",
        "\n",
        "The **sentence polarity dataset v1.0** contains two data files which are: \n",
        "  * **rt-polarity.pos**: It contains 5331 positive examples\n",
        "  * **rt-polarity.neg**: It contains 5331 negative examples\n",
        "\n",
        "Each line in these two files corresponds to a single snippet (usually\n",
        "containing roughly one single sentence) that includes the review of a movie. \n",
        "\n",
        "**Note:** Here is the source [link](https://www.cs.cornell.edu/people/pabo/movie-review-data/) to the Movie  dataset\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7C-eklopT0q0"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "The aim of this assignment is to study the use of multi-layer bidirectional GRU (Gated\n",
        "Recurrent Unit) for solving the sentence classification problem. You will study the effect of adding\n",
        "layers of BiGRU units on the test set performance of the model. The dataset used will be the same\n",
        "as that was used in the first assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OezYgKMk11WU"
      },
      "source": [
        "### Importing the libraries and packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords \n",
        "import string\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from keras.layers import Input, Embedding, Dense, Bidirectional, Dropout, GRU\n",
        "from keras.models import Sequential   # the model"
      ],
      "metadata": {
        "id": "ZKep6MOP3NHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8nF44dZ14eG"
      },
      "source": [
        "### Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the positive and negative files and split the sentences into a list\n",
        "with open('rt-polarity.neg',\"r\") as data_neg:\n",
        "  data_neg_set = data_neg.read().splitlines()\n",
        "\n",
        "with open('rt-polarity.pos',\"r\") as data_pos:\n",
        "  data_pos_set = data_pos.read().splitlines()"
      ],
      "metadata": {
        "id": "_fzYuJSqCPGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the positive and negative reviews\n",
        "len(data_neg_set), len(data_pos_set)"
      ],
      "metadata": {
        "id": "CM-TmdP0HG4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the negative reviews\n",
        "data_neg_set = pd.DataFrame(data_neg_set, columns=[\"Review\"])\n",
        "\n",
        "# Loading the positive reviews\n",
        "data_pos_set = pd.DataFrame(data_pos_set, columns=[\"Review\"])"
      ],
      "metadata": {
        "id": "umug6OCKkXAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first five rows of the positive examples\n",
        "data_pos_set.head()"
      ],
      "metadata": {
        "id": "mQk2GX8JkXAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first five rows of the negative examples\n",
        "data_neg_set.head()"
      ],
      "metadata": {
        "id": "kwG0O-fSkXAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJh13HlV16WC"
      },
      "source": [
        "#### Giving the labels to the data\n",
        "\n",
        "Let us give the labels as positive and negative for the sentences present in the two files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G95kzywrfCIJ"
      },
      "source": [
        "data_neg_set['Polarity'] = 'Negative'\n",
        "data_pos_set['Polarity'] = 'Positive'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tawtWMsWOzzU"
      },
      "source": [
        "Let us have a glance at few of the values present in the data with negative and positive reviews that we have labeled in the previous step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_RweIcy-Elq"
      },
      "source": [
        "data_neg_set.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fftKy58R2BQ7"
      },
      "source": [
        "#### Combining the positive and negative data\n",
        "\n",
        "Now, we have to work on the combined data containing the positive and negative reviews, so, let us concatenate both the dataframes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDNpIJTwfoju"
      },
      "source": [
        "dataframes = [data_neg_set, data_pos_set]\n",
        "rt_polarity_data = pd.concat(dataframes)\n",
        "rt_polarity_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fErTZ4xY_ag0"
      },
      "source": [
        "\n",
        "From above we can see that due to concatenation the last row index is 5330 that we can see below but that should be 10661."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI1Q71Gsi0m6"
      },
      "source": [
        "Therefore, we will reset the index so that last row has index of 10661."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rt_polarity_data.reset_index(inplace=True, drop=True)"
      ],
      "metadata": {
        "id": "0XhcLPvJkugX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rt_polarity_data"
      ],
      "metadata": {
        "id": "nl1VpjBPUghV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXeLwOabUfRI"
      },
      "source": [
        "When you combine the negative and positive examples, it is a good idea to shuffle the examples so that the negative and positive examples are spread throughout. If we do not shuffle it, then, it may happen that in some mini-batches, examples from only one class(positive or negative) will be present. Therefore, it is better to avoid such scenarios.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rt_polarity_data = shuffle(rt_polarity_data)"
      ],
      "metadata": {
        "id": "P0nQbQmOI16K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rt_polarity_data"
      ],
      "metadata": {
        "id": "ZrUaqqNjqeTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrVrS5SmP2-z"
      },
      "source": [
        "Let us check the value counts of negative and positive reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiXfMlw8iWct"
      },
      "source": [
        "rt_polarity_data['Polarity'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aK3WHmukP-Ya"
      },
      "source": [
        "Checking whether there are any null values present in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGk4tTa3bcrj"
      },
      "source": [
        "rt_polarity_data.isnull().values.any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RbrsSj2yJAt"
      },
      "source": [
        "### Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0HUwo3qRhJo"
      },
      "source": [
        "# Converting the labels from categorical to numerical\n",
        "le = LabelEncoder()\n",
        "rt_polarity_data['Polarity'] = le.fit_transform(rt_polarity_data['Polarity'])\n",
        "rt_polarity_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMIU5YtCv0k8"
      },
      "source": [
        "### Data Preprocessing \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VykHHDNwE7zD"
      },
      "source": [
        "We can preprocess the text using gensim package. Gensim provides function **simple_preprocess** for more effective preprocessing of the corpus. In such kind of preprocessing, we can convert a document into a list of lowercase tokens. We can also ignore tokens that are too short or too long.\n",
        "\n",
        "**Note:** Refer to the following [link](https://radimrehurek.com/gensim/utils.html#gensim.utils.simple_preprocess) for gensim `simple_preprocess` method"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rt_polarity_data['Review'] = rt_polarity_data['Review'].apply(lambda x:simple_preprocess(x, max_len=30))"
      ],
      "metadata": {
        "id": "kbqIykM7Y7Au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "rt_polarity_data['Review'] = rt_polarity_data['Review'].apply(lambda x: [w for w in x if not w in stop_words])"
      ],
      "metadata": {
        "id": "bCEarqTCZShf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rt_polarity_data.head()"
      ],
      "metadata": {
        "id": "XFhCoAI1ZbYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters"
      ],
      "metadata": {
        "id": "01GaXtj3nvo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters \n",
        "MAX_SENT_LEN = 30   # Number of words to consider from each review\n",
        "MAX_VOCAB_SIZE = 20000  # Max vocabulary size\n",
        "BATCH_SIZE = 32\n",
        "N_EPOCHS = 15"
      ],
      "metadata": {
        "id": "TNzXbT_YUq6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BbSPU936D0R"
      },
      "source": [
        "### Tokenize and Pad sequences\n",
        "\n",
        "A Neural Network only accepts numeric data, so we need to encode the reviews. Here use keras.Tokenizer() to encode the reviews into integers, where each unique word is automatically indexed (using `fit_on_texts` method) calculates the frequency of each word in our corpus/messages. \n",
        "\n",
        "`texts_to_sequences` method finally converts our array of sequences of strings to list of sequences of integers (most frequent word is assigned 1 and so on).\n",
        "\n",
        "Each reviews has a different length, so we need to add padding (by adding 0) or truncating the words to the same length (in this case, it is the mean of all reviews length) using `keras.preprocessing.sequence.pad_sequences.`\n",
        "\n",
        "`post`, pad or truncate the words in the back of a sentence\n",
        "`pre`, pad or truncate the words in front of a sentence\n",
        "\n",
        "Each word is assigned an integer and that integer is placed in a list. \n",
        "\n",
        "\n",
        "For example if we have a sentence “How text to sequence and padding works”. Each word is assigned a number. We suppose how = 1, text = 2, to = 3, sequence = 4, and = 5, padding = 6, works = 7. After texts_to_sequences is called our sentence will look like [1, 2, 3, 4, 5, 6, 7 ]. Now for suppose our MAX_SEQUENCE_LENGTH = 10. After padding our sentence will look like `pre` = [0, 0, 0, 1, 2, 3, 4, 5, 6, 7 ], `post` = [1, 2, 3, 4, 5, 6, 7, 0, 0, 0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts([' '.join(seq[:MAX_SENT_LEN]) for seq in rt_polarity_data['Review']])\n",
        "\n",
        "print(\"Number of words in vocabulary:\", len(tokenizer.word_index))"
      ],
      "metadata": {
        "id": "OMFB2HFDU0aW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the sequence of words to sequnce of indices\n",
        "X = tokenizer.texts_to_sequences([' '.join(seq[:MAX_SENT_LEN]) for seq in rt_polarity_data['Review']])\n",
        "X = pad_sequences(X, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
        "\n",
        "y = rt_polarity_data['Polarity']"
      ],
      "metadata": {
        "id": "hQj1GNFxI-7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4P8bwYey5MD"
      },
      "source": [
        "### Splitting the data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42, train_size=10000)"
      ],
      "metadata": {
        "id": "9y4sQ5KUWvgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "id": "pEcGKY3RoP_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xMZ9Hb1l-gd"
      },
      "source": [
        "### Load the GloVe word embeddings\n",
        "\n",
        "**What is GloVe?**\n",
        "\n",
        "GloVe stands for global vectors for word representation. It is an unsupervised learning algorithm developed by Stanford for generating word embeddings by aggregating global word-word co-occurrence matrix from a corpus. Word embeddings are basically a form of word representation that bridges the human understanding of language to that of a machine. Meaning that two similar words are represented by almost similar vectors that are very closely placed in a vector space. These are essential for solving most Natural language processing problems.The resulting embeddings show interesting linear substructures of the word in vector space.\n",
        "\n",
        "Thus when using word embeddings, all individual words are represented as real-valued vectors in a predefined vector space. Each word is mapped to one vector and the vector values are learned in a way that resembles a neural network.\n",
        "\n",
        "Now, let us load the 300-dimensional GloVe embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNHXa8G4DMcm"
      },
      "source": [
        "embeddings_index = {}\n",
        "# Loading the 300-dimensional vector of the model\n",
        "f = open('/content/glove.6B.300d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbUbiAQ1D5HI"
      },
      "source": [
        "# Adding 1 because of reversed 0 index\n",
        "words_not_found = []\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "embedding_dim = 300\n",
        "\n",
        "# Create a weight matrix for words in the training data\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i >= vocab_size:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        words_not_found.append(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoOwxlsFEFdH"
      },
      "source": [
        "print(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQmtuo7MEIm4"
      },
      "source": [
        "print(len(tokenizer.word_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4wid6RBzVEs"
      },
      "source": [
        "### Define the Bi-directional GRU model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maNZlRA4PGTO"
      },
      "source": [
        "### LSTM vs GRU\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/GRU.png\" width=700px, height=500/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "Simple RNNs have a very short memory, due to the issue of vanishing gradients. More complicated cell architectures try to solve the short memory problem. The most famous one is probably the Long Short Term Memory (LSTM) cell:\n",
        "\n",
        "\n",
        "It uses a gated cell architecture to update and forget information selectively in the network memory (cell and hidden states). The Gated Recurrent Units (GRU) have a slightly simpler architecture (and only one hidden state). GRUs are usually faster than LSTMs, while still often have competitive performances for many applications.\n",
        "\n",
        "### GRU - The subtle differences\n",
        "\n",
        "* The **update gate** acts similar to the **forget gate** and **input gate** of an LSTM\n",
        "\n",
        "* The **update gate** decides how much of the past information (from previous time steps) needs to be passed along to the future.\n",
        "\n",
        "* The **reset gate** decides how much of the past information to forget\n",
        "\n",
        "* Some tensor ops and speedier to train than LSTMs\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/GRU_Subtle_difference.png\" width=700px, height=500/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "### The need for Bi-directional GRUs\n",
        "\n",
        "* Bi-directional GRUs are just putting two independent GRUs together\n",
        "\n",
        "* The input sequence is fed in forward order for one GRU, and reverse order for the other\n",
        "\n",
        "* The otputs of the two networks are usually concatenated at each time step\n",
        "\n",
        "* Preserving information from both past and future helps understand context better\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/Bi-GRU.jpg\" width=700px, height=500/>\n",
        "</center>\n",
        "\n",
        "\n",
        "A bidirectional GRU consists of forward layer and a backward layer. The input sequence is fed to the forward layer in the regular way, while in the backward layer the input is processed in the reverse order, starting from the last word, then proceed to the next to last word and so on up to to first word.\n",
        "\n",
        "The hidden states are then concatenated for each token generating an intermediate representation sequence. Hence, for each intermediate representation the information from the sequence before and after the respective token are taken into account. That means for each iteration step the network has access to the complete document and can deduce the right label from that information."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a sequential model by stacking neural net units \n",
        "model = Sequential()\n",
        "embedding_layer = Embedding(vocab_size,\n",
        "                            embedding_dim, \n",
        "                            weights = [embedding_matrix],\n",
        "                            input_length = MAX_SENT_LEN,\n",
        "                            trainable=False)\n",
        "model.add(embedding_layer)\n",
        "model.add(Bidirectional(GRU(128, return_sequences=True, dropout=0.50, name='first_gru_layer')))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Bidirectional(GRU(64, name='second_gru_layer')))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid', name='output_layer'))"
      ],
      "metadata": {
        "id": "RYSb839kVW65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Summary of the built model...')\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "_Cv29I2cV2mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8wo9lrl1PKC"
      },
      "source": [
        "### Compile and train the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "T1FjC0r_V1i4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=N_EPOCHS,\n",
        "          validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "bAi9pOu_V9cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate the model"
      ],
      "metadata": {
        "id": "WWq5Xcx9pI5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Testing...')\n",
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "7HrSFMxGPUlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model predictions on the test data\n",
        "preds = model.predict(X_test)"
      ],
      "metadata": {
        "id": "XqifBMb1Pw9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds.shape"
      ],
      "metadata": {
        "id": "WsuHq-Iaysfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the text sequences for the preprocessed movie reviews\n",
        "reviews_list_idx = tokenizer.texts_to_sequences([' '.join(seq[:MAX_SENT_LEN]) for seq in rt_polarity_data['Review']])"
      ],
      "metadata": {
        "id": "eUgCJNexzRQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(reviews_list_idx[1])"
      ],
      "metadata": {
        "id": "Tr2asGbqz0Qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uO43uYffCfs"
      },
      "source": [
        "# Function to get the predictions on the movie reviews using GRU model\n",
        "def add_score_predictions(data, reviews_list_idx):\n",
        "\n",
        "  # Pad the sequences of the data\n",
        "  reviews_list_idx = pad_sequences(reviews_list_idx, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
        "  \n",
        "  # Get the predictons by using GRU model\n",
        "  review_preds = model.predict(reviews_list_idx)\n",
        "  \n",
        "  # Add the predictions to the movie reviews data\n",
        "  rt_polarity_data['polarity score'] = review_preds\n",
        "  \n",
        "  # Set the threshold for the predictions\n",
        "  pred_sentiment = np.array(list(map(lambda x : 'positive' if x > 0.5 else 'negative', review_preds)))\n",
        "\n",
        "  # Add the sentiment predictions to the movie reviews\n",
        "  rt_polarity_data['predicted polarity'] = pred_sentiment\n",
        "\n",
        "  return rt_polarity_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pes81oz7hQpx"
      },
      "source": [
        "# Call the above function to get the sentiment score and the predicted sentiment\n",
        "data = add_score_predictions(rt_polarity_data, reviews_list_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtZbstE0i2cr"
      },
      "source": [
        "# Display the data\n",
        "data[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EBa9MGLIgN4"
      },
      "source": [
        "**Consider the following statements and answer Q1:**\n",
        "\n",
        "\n",
        "A. The GRU controls the past information by having two gates: an update gate and reset gate, the update gate determines how much of the past knowledge needs to be passed along into the future and the reset gate is used to decide how much of the past information to forget.\n",
        "\n",
        "B. GRUs handles the vanishing gradient problem in recurrent neural networks very efficiently.\n",
        "\n",
        "C. GRUs cannot handle the vanishing gradient problem in recurrent neural networks very efficiently.\n",
        "\n",
        "D. GRUs solve the problem of exploding gradients using gradient clipping in which a defined threshold value is set on the gradients, which means that even if a gradient increases beyond the predefined value during training, its value will still be limited to the set threshold.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgSwVENIPcM6"
      },
      "source": [
        "#@title Q1. Which of the above statement(s) is/are True? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer1 = \"A, B and D\" #@param [\"\", \"Only A and B\", \"A, B and D\", \"Only C\", \"only A\", \"only D\"]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeMY9dvfYTgu"
      },
      "source": [
        "**Consider the following statements and answer Q2:**\n",
        "\n",
        "\n",
        "A. Bidirectional GRU, or BiGRU, is a sequence processing model, consisting of two GRUs, one taking the input in a forward direction, and the other in a backwards direction.\n",
        "\n",
        "B. Bidirectional GRUs are  a type of bidirectional recurrent neural networks with only forget gates.\n",
        "\n",
        "C. GRUs use less training parameters and therefore use less memory and execute faster and generally have a better performance than LSTMs when dealing with smaller datasets.\n",
        "\n",
        "D. For GRU bidirectional layer output, we take the sum of outputs of both forward and backward direction layers\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oik-XuZJKUOh"
      },
      "source": [
        "#@title Q2. Which of the above statement(s) is/are True? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer2 = \"A and C\" #@param [\"\", \"A and B\",\"A and C\",\"B and D\",\"C and D\", \"only A\", \"only C\"]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"Good and Challenging for me\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"NA\" #@param {type:\"string\"}\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"Yes\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"Somewhat Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"Somewhat Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-",
        "outputId": "a9a2b9b2-29f2-4922-85ea-99ee52414a24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your submission is successful.\n",
            "Ref Id: 1635\n",
            "Date of submission:  16 May 2023\n",
            "Time of submission:  14:53:36\n",
            "View your submissions: https://dlfa-iisc.talentsprint.com/notebook_submissions\n"
          ]
        }
      ]
    }
  ]
}